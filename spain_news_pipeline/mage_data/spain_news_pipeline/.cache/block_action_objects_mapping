{"block_file": {"data_exporters/export_titanic_clean.py:data_exporter:python:export titanic clean": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to filesystem.\n\n    Docs: https://docs.mage.ai/design/data-loading#example-loading-data-from-a-file\n    \"\"\"\n    filepath = 'titanic_clean.csv'\n    FileIO().export(df, filepath)\n", "file_path": "data_exporters/export_titanic_clean.py", "language": "python", "type": "data_exporter", "uuid": "export_titanic_clean"}, "data_loaders/fetch_xml_using_feedparser.py:data_loader:python:fetch xml using feedparser": {"content": "import feedparser\nimport argparse\nfrom datetime import datetime\n\n\nimport pandas as pd\n\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nURL_1 = \"https://news.google.com/rss/search?q=Espa%C3%B1a&hl=es&gl=ES&ceid=ES%3Aes\"\nURL_2 = \"https://news.google.com/rss/search?q=Spain&hl=en-US&gl=US&ceid=US%3Aen\"\n\n\n@data_loader\ndef fetch_raw_data(*args, **kwargs) -> list[dict]:\n    \"\"\"_summary_ : This function fetches raw data from the inputed url\n\n    Args:\n        url (string): _description_\n    Returns:\n        list[dict]\n    \"\"\"    \n    args = parser.parse_args()\n    url = args.url\n    data = feedparser.parse(url)['entries']\n    \n    return data\n\n\n@test\ndef test_raw_data(output) -> None:\n    assert len(output) > 0, \"Empty output object\" \n    assert isinstance(output, list), f\"Output is not a list, {type(output)}\"\n    assert isinstance(output[0], dict), f\"Output is not a dictionary, {type(output[0])}\"\n", "file_path": "data_loaders/fetch_xml_using_feedparser.py", "language": "python", "type": "data_loader", "uuid": "fetch_xml_using_feedparser"}, "data_loaders/load_titanic.py:data_loader:python:load titanic": {"content": "import io\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv?raw=True'\n\n    return pd.read_csv(url)\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_titanic.py", "language": "python", "type": "data_loader", "uuid": "load_titanic"}, "transformers/fill_in_missing_values.py:transformer:python:fill in missing values": {"content": "from pandas import DataFrame\nimport math\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef select_number_columns(df: DataFrame) -> DataFrame:\n    return df[['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Survived']]\n\n\ndef fill_missing_values_with_median(df: DataFrame) -> DataFrame:\n    for col in df.columns:\n        values = sorted(df[col].dropna().tolist())\n        median_value = values[math.floor(len(values) / 2)]\n        df[[col]] = df[[col]].fillna(median_value)\n    return df\n\n\n@transformer\ndef transform_df(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        df (DataFrame): Data frame from parent block.\n\n    Returns:\n        DataFrame: Transformed data frame\n    \"\"\"\n    # Specify your transformation logic here\n\n    return fill_missing_values_with_median(select_number_columns(df))\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "transformers/fill_in_missing_values.py", "language": "python", "type": "transformer", "uuid": "fill_in_missing_values"}, "pipelines/spain_news_pipeline/__init__.py:pipeline:python:spain news pipeline/  init  ": {"content": "", "file_path": "pipelines/spain_news_pipeline/__init__.py", "language": "python", "type": "pipeline", "uuid": "spain_news_pipeline/__init__"}, "pipelines/spain_news_pipeline/metadata.yaml:pipeline:yaml:spain news pipeline/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: fetch xml using feedparser\n  retry_config: null\n  status: failed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: fetch_xml_using_feedparser\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-12-31 23:23:34.731474+00:00'\ndata_integration: null\ndescription: Automated pipeline to grab news from google on Spain and assess the sentiment\n  of the news.\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: local_python\nextensions: {}\nname: spain_news_pipeline\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags:\n- python\ntype: python\nuuid: spain_news_pipeline\nvariables_dir: /home/src/mage_data/spain_news_pipeline\nwidgets: []\n", "file_path": "pipelines/spain_news_pipeline/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "spain_news_pipeline/metadata"}, "pipelines/example_pipeline/__init__.py:pipeline:python:example pipeline/  init  ": {"content": "", "file_path": "pipelines/example_pipeline/__init__.py", "language": "python", "type": "pipeline", "uuid": "example_pipeline/__init__"}, "pipelines/example_pipeline/metadata.yaml:pipeline:yaml:example pipeline/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  downstream_blocks:\n  - fill_in_missing_values\n  name: load_titanic\n  status: not_executed\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_titanic\n- all_upstream_blocks_executed: true\n  downstream_blocks:\n  - export_titanic_clean\n  name: fill_in_missing_values\n  status: not_executed\n  type: transformer\n  upstream_blocks:\n  - load_titanic\n  uuid: fill_in_missing_values\n- all_upstream_blocks_executed: true\n  downstream_blocks: []\n  name: export_titanic_clean\n  status: not_executed\n  type: data_exporter\n  upstream_blocks:\n  - fill_in_missing_values\n  uuid: export_titanic_clean\nname: example_pipeline\ntype: python\nuuid: example_pipeline\nwidgets: []\n", "file_path": "pipelines/example_pipeline/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "example_pipeline/metadata"}, "/home/src/spain_news_pipeline/data_loaders/fetch_xml_using_feedparser.py:data_loader:python:home/src/spain news pipeline/data loaders/fetch xml using feedparser": {"content": "import feedparser\nimport pandas as pd\nfrom datetime import datetime\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n# Defined globally for easy access\nURLS = [\n    \"https://news.google.com/rss/search?q=Espa%C3%B1a&hl=es&gl=ES&ceid=ES%3Aes\",\n    \"https://news.google.com/rss/search?q=Spain&hl=en-US&gl=US&ceid=US%3Aen\"\n]\n\n@data_loader\ndef fetch_raw_data(*args, **kwargs) -> list:\n    \"\"\"\n    Fetches raw data from the Google News RSS feeds for Spain.\n    \"\"\"    \n    all_entries = []\n    \n    for url in URLS:\n        print(f\"Fetching data from: {url}\")\n        feed = feedparser.parse(url)\n        \n        # Adding entries to our master list\n        if feed.entries:\n            all_entries.extend(feed.entries)\n    \n    print(f\"Total entries fetched: {len(all_entries)} {type(all_entries)}\")\n    return {'entries':all_entries}\n\n\n@test\ndef test_raw_data(output) -> None:\n\n    data = output.get('entries')\n\n    assert data is not None, \"The output is None\"\n    assert len(data) > 0, \"Empty output object\" \n    assert isinstance(data, list), f\"Output is not a list, it is {type(output)}\"\n    \n    # Check if the first item is a dictionary-like object (feedparser uses AttrDict)\n    assert isinstance(data[0], dict) or hasattr(data[0], 'keys'), \"Items are not dicts\"", "file_path": "/home/src/spain_news_pipeline/data_loaders/fetch_xml_using_feedparser.py", "language": "python", "type": "data_loader", "uuid": "fetch_xml_using_feedparser"}, "/home/src/spain_news_pipeline/transformers/parse_data_into_dataframes.py:transformer:python:home/src/spain news pipeline/transformers/parse data into dataframes": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test \n\nimport pandas as pd\nfrom datetime import datetime\n\n@transformer\ndef transform(data, *args, **kwargs):\n    \"\"\"\n    Convert the output from parent block into a dataframe object\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Returns:\n        data frame\n    \"\"\"\n    processed_data = []\n    if isinstance(data, list):\n        all_entries = data\n    elif isinstance(data, dict):\n        all_entries = data['entries']\n    else:\n        all_entries = [data] + list(args)\n\n    for entry in all_entries:\n        if not isinstance(entry, dict):\n            continue\n\n        row = {\n            'title':entry.get('title'),\n            'published_date': pd.to_datetime(entry.get('published')),\n            'summary': entry.get('summary'),\n            'source_name': entry['source']['title'],\n            'source_url': entry['source']['href'],\n            'entry_date': pd.to_datetime(datetime.now())\n        }\n        processed_data.append(row)\n\n    return pd.DataFrame(processed_data)\n\n\n@test\ndef test_output(output, *args) -> None:\n    assert output is not None, 'The output is undefined'\n    assert isinstance(output, pd.DataFrame), f\"output has not been parsed ... currently {type(output)}\"\n", "file_path": "/home/src/spain_news_pipeline/transformers/parse_data_into_dataframes.py", "language": "python", "type": "transformer", "uuid": "parse_data_into_dataframes"}, "/home/src/spain_news_pipeline/transformers/convert_the__title__into_english_from_spanish.py:transformer:python:home/src/spain news pipeline/transformers/convert the  title  into english from spanish": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nimport pandas as pd\nimport torch \nfrom transformers import pipeline\n \n@transformer\ndef transform(data, *args, **kwargs):\n    assert isinstance(data, pd.DataFrame), \"data is not a dataframe!!\"\n    \n    # 1. FIX: Use the correct model for TRANSLATION\n    print(\"Loading translation model (Helsinki-NLP ES -> EN)...\")\n    pipeline_translation = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\")\n\n    # 2. Loading the Sentiment Analyzers\n    print(\"Loading English sentiment analyzer (DistilBERT)...\")\n    pipeline_sentiment_en = pipeline(\"sentiment-analysis\", model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n    \n    print(\"Loading Spanish sentiment analyzer (Robertuito)...\")\n    pipeline_sentiment_es = pipeline(\"sentiment-analysis\", model=\"pysentimiento/robertuito-sentiment-analysis\")\n\n    # Clean titles (Removing source after the hyphen)\n    titles_es = data['title'].str.rsplit(\"-\", n=1).str[0].str.strip().to_list()\n\n    # TRANSLATION\n    print(f\"Translating {len(titles_es)} items...\")\n    translations = pipeline_translation(titles_es, truncation=True)\n    # Helsinki-NLP returns a list of dicts with 'translation_text'\n    data['title_english'] = [t['translation_text'] for t in translations]\n\n    # SENTIMENT ANALYSIS\n    print(\"Analyzing sentiment (English and Spanish)...\")\n    \n    # Run English sentiment on translated titles\n    sent_en_results = pipeline_sentiment_en(data[\"title_english\"].to_list(), truncation=True)\n    \n    # Run Spanish sentiment on original (cleaned) titles\n    sent_es_results = pipeline_sentiment_es(titles_es, truncation=True)\n    \n    # Map results for English model\n    data['sentiment_label_en'] = [s['label'] for s in sent_en_results]\n    data['sentiment_score_en'] = [s['score'] for s in sent_en_results] \n\n    # Map results for Spanish model (Robertuito)\n    label_map = {'POS': 'Positive', 'NEU': 'Neutral', 'NEG': 'Negative'}\n    data['sentiment_label_es'] = [label_map.get(s['label'], s['label']) for s in sent_es_results]\n    data['sentiment_score_es'] = [s['score'] for s in sent_es_results]\n\n    return data\n\n@test\ndef test_output(output, *args) -> None:\n    assert 'title_english' in output.columns, 'Translation column missing'\n    assert 'sentiment_label_es' in output.columns, 'Spanish Sentiment column missing'", "file_path": "/home/src/spain_news_pipeline/transformers/convert_the__title__into_english_from_spanish.py", "language": "python", "type": "transformer", "uuid": "convert_the__title__into_english_from_spanish"}, "/home/src/spain_news_pipeline/data_exporters/save_to_duckdb.py:data_exporter:python:home/src/spain news pipeline/data exporters/save to duckdb": {"content": "if 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\nimport duckdb\nimport pandas as pd\n\n@data_exporter\ndef export_data(data, *args, **kwargs):\n    \"\"\"\n    Exports data to local duckdb database file\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Output (optional):\n        Optionally return any object and it'll be logged and\n        displayed when inspecting the block run.\n    \"\"\"\n    # 1. Connect to file\n    conn = duckdb.connect(\"news_db.duckdb\")\n\n    #2. Create table if it doesn't exist\n    conn.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS news (\n            title TEXT, \n            published_date TIMESTAMP, \n            summary TEXT, \n            source_name TEXT, \n            source_url TEXT, \n            entry_date DATETIME, \n            title_english TEXT, \n            sentiment_label_en TEXT, \n            sentiment_score_en FLOAT, \n            sentiment_label_es TEXT, \n            sentiment_score_es FLOAT\n        );\n    \"\"\")\n\n    conn.execute(\"CREATE TEMP TABLE new_batch AS SELECT * FROM data\")\n    conn.execute(\"\"\"\n        INSERT INTO news\n        SELECT * FROM new_batch\n        WHERE source_url NOT IN (SELECT source_url FROM news)\n    \"\"\")\n\n    conn.close()", "file_path": "/home/src/spain_news_pipeline/data_exporters/save_to_duckdb.py", "language": "python", "type": "data_exporter", "uuid": "save_to_duckdb"}, "/home/src/spain_news_pipeline/scratchpads/get_the_creation_sql.py:scratchpad:python:home/src/spain news pipeline/scratchpads/get the creation sql": {"content": "from mage_ai.data_preparation.variable_manager import get_variable\nfrom sqlalchemy import create_engine\nimport pandas as pd \n\n# Retrieve the dataframe from your AI sentiment block\ndf = get_variable('spain_news_pipeline', 'convert_the__title__into_english_from_spanish', 'output_0')\n\n# Create the engine\nengine = create_engine('sqlite:///')\n\n# Get the SQL schema\ncreate_table_sql = pd.io.sql.get_schema(df, 'spain_news_monitor', con=engine)\n\nprint(f\"\"\"\n---- DATA SCHEMA ---- \n{create_table_sql}\n\"\"\")", "file_path": "/home/src/spain_news_pipeline/scratchpads/get_the_creation_sql.py", "language": "python", "type": "scratchpad", "uuid": "get_the_creation_sql"}, "/home/src/spain_news_pipeline/scratchpads/check_if_data_exists_in_table.py:scratchpad:python:home/src/spain news pipeline/scratchpads/check if data exists in table": {"content": "\"\"\"\nNOTE: Scratchpad blocks are used only for experimentation and testing out code.\nThe code written here will not be executed as part of the pipeline.\n\"\"\"\n\nfrom mage_ai.data_preparation.variable_manager import get_variable\nfrom sqlalchemy import create_engine\nimport pandas as pd\n\n\n# 1. Get data\ndf = get_variable('spain_news_pipeline', \n                  'convert_the__title__into_english_from_spanish', \n                  'output_0')\n\n#2. Create fresh connection engine\nengine = create_engine('sqlite:///')\n\n#3. create table and load the data\ndf.to_sql('spain_news_monitor',\n          con = engine,\n          index=False,\n          if_exists='replace')\n\n# run the query\nquery = \"\"\"\n    select \n        title,\n        sentiment_label_en,\n        sentiment_label_es,\n        sentiment_score_en,\n        sentiment_score_es\n    from spain_news_monitor\n    where sentiment_label_es = 'Negative'\n    order by sentiment_score_en DESC\n    limit 10\n\"\"\"\n\n# Execute and look at the results\nresulting_df = pd.read_sql(query,\n                           con=engine)\n\nprint(f\"\"\"\n\\t\\t\\t---- QUERY RESULTS ---- \\n\\n\n{resulting_df}\n\"\"\")", "file_path": "/home/src/spain_news_pipeline/scratchpads/check_if_data_exists_in_table.py", "language": "python", "type": "scratchpad", "uuid": "check_if_data_exists_in_table"}, "/home/src/spain_news_pipeline/scratchpads/scratchpad_01.py:scratchpad:python:home/src/spain news pipeline/scratchpads/scratchpad 01": {"content": "\"\"\"\nNOTE: Scratchpad blocks are used only for experimentation and testing out code.\nThe code written here will not be executed as part of the pipeline.\n\"\"\"\n\nfrom mage_ai.data_preparation.variable_manager import get_variable\nfrom sqlalchemy import create_engine\nimport pandas as pd\n\n\n# 1. Get data\ndf = get_variable('spain_news_pipeline', \n                  'convert_the__title__into_english_from_spanish', \n                  'output_0')\n\n#2. Create fresh connection engine\nengine = create_engine('sqlite:///')\n\n#3. create table and load the data\ndf.to_sql('spain_news_monitor',\n          con = engine,\n          index=False,\n          if_exists='replace')\n\n# run the query | Get the overall vibe \nquery = \"\"\"\n    select \n        sentiment_label_es as sentiment,\n        published_date,\n        count(*) as count\n    from spain_news_monitor\n    group by sentiment_label_es\n    order by published_date\n\"\"\"\n\n# Execute and look at the results\nresulting_df = pd.read_sql(query,\n                           con=engine)\n\nprint(f\"\"\"\n\\t\\t\\t---- QUERY RESULTS ---- \\n\n{resulting_df}\n\"\"\")", "file_path": "/home/src/spain_news_pipeline/scratchpads/scratchpad_01.py", "language": "python", "type": "scratchpad", "uuid": "scratchpad_01"}, "/home/src/spain_news_pipeline/scratchpads/scratchpad__02.py:scratchpad:python:home/src/spain news pipeline/scratchpads/scratchpad  02": {"content": "\"\"\"\nNOTE: Scratchpad blocks are used only for experimentation and testing out code.\nThe code written here will not be executed as part of the pipeline.\n\"\"\"\n\nfrom mage_ai.data_preparation.variable_manager import get_variable\nfrom sqlalchemy import create_engine\nimport pandas as pd\n\n\n# 1. Get data\ndf = get_variable('spain_news_pipeline', \n                  'convert_the__title__into_english_from_spanish', \n                  'output_0')\n\n#2. Create fresh connection engine\nengine = create_engine('sqlite:///')\n\n#3. create table and load the data\ndf.to_sql('spain_news_monitor',\n          con = engine,\n          index=False,\n          if_exists='replace')\n\n# run the query | News mentioning Barcelona or Madrid or Valencia or Oil or energy\nquery = \"\"\"\n    select \n        title_english,\n        sentiment_label_es,\n        sentiment_score_es\n    from spain_news_monitor\n    where title like '%Barcelona%' or title like '%Madrid' or title like '%Valencia%' or title like '%Oil%' or title like '%energy%'\n\"\"\"\n\n# Execute and look at the results\nresulting_df = pd.read_sql(query,\n                           con=engine)\n\nprint(f\"\"\"\n\\t\\t\\t---- QUERY RESULTS ---- \\n\\n\n{resulting_df}\n\"\"\")", "file_path": "/home/src/spain_news_pipeline/scratchpads/scratchpad__02.py", "language": "python", "type": "scratchpad", "uuid": "scratchpad__02"}}, "custom_block_template": {}, "mage_template": {"data_loaders/airtable.py:data_loader:python:Airtable:Load a Table from Airtable App.": {"block_type": "data_loader", "description": "Load a Table from Airtable App.", "language": "python", "name": "Airtable", "path": "data_loaders/airtable.py"}, "data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}